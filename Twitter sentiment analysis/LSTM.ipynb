{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c493587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "                                              0.0/24.0 MB ? eta -:--:--\n",
      "                                             0.0/24.0 MB 660.6 kB/s eta 0:00:37\n",
      "                                             0.1/24.0 MB 657.6 kB/s eta 0:00:37\n",
      "                                             0.2/24.0 MB 893.0 kB/s eta 0:00:27\n",
      "                                              0.3/24.0 MB 1.1 MB/s eta 0:00:23\n",
      "                                              0.4/24.0 MB 1.2 MB/s eta 0:00:21\n",
      "                                              0.4/24.0 MB 1.2 MB/s eta 0:00:20\n",
      "                                              0.6/24.0 MB 1.3 MB/s eta 0:00:19\n",
      "     -                                        0.7/24.0 MB 1.5 MB/s eta 0:00:16\n",
      "     -                                        1.0/24.0 MB 1.9 MB/s eta 0:00:13\n",
      "     --                                       1.4/24.0 MB 2.2 MB/s eta 0:00:11\n",
      "     --                                       1.7/24.0 MB 2.5 MB/s eta 0:00:09\n",
      "     ---                                      2.0/24.0 MB 2.7 MB/s eta 0:00:09\n",
      "     ---                                      2.2/24.0 MB 2.7 MB/s eta 0:00:09\n",
      "     ----                                     2.4/24.0 MB 2.8 MB/s eta 0:00:08\n",
      "     ----                                     2.7/24.0 MB 2.9 MB/s eta 0:00:08\n",
      "     -----                                    3.0/24.0 MB 3.1 MB/s eta 0:00:07\n",
      "     -----                                    3.3/24.0 MB 3.1 MB/s eta 0:00:07\n",
      "     -----                                    3.6/24.0 MB 3.2 MB/s eta 0:00:07\n",
      "     ------                                   3.9/24.0 MB 3.3 MB/s eta 0:00:07\n",
      "     ------                                   4.1/24.0 MB 3.3 MB/s eta 0:00:06\n",
      "     -------                                  4.4/24.0 MB 3.4 MB/s eta 0:00:06\n",
      "     -------                                  4.5/24.0 MB 3.3 MB/s eta 0:00:06\n",
      "     --------                                 4.9/24.0 MB 3.5 MB/s eta 0:00:06\n",
      "     --------                                 5.2/24.0 MB 3.5 MB/s eta 0:00:06\n",
      "     ---------                                5.5/24.0 MB 3.5 MB/s eta 0:00:06\n",
      "     ---------                                5.8/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "     ----------                               6.1/24.0 MB 3.7 MB/s eta 0:00:05\n",
      "     ----------                               6.5/24.0 MB 3.7 MB/s eta 0:00:05\n",
      "     -----------                              6.7/24.0 MB 3.7 MB/s eta 0:00:05\n",
      "     -----------                              7.1/24.0 MB 3.8 MB/s eta 0:00:05\n",
      "     ------------                             7.4/24.0 MB 3.8 MB/s eta 0:00:05\n",
      "     ------------                             7.7/24.0 MB 3.9 MB/s eta 0:00:05\n",
      "     -------------                            8.0/24.0 MB 3.9 MB/s eta 0:00:05\n",
      "     -------------                            8.2/24.0 MB 3.9 MB/s eta 0:00:05\n",
      "     --------------                           8.5/24.0 MB 3.9 MB/s eta 0:00:04\n",
      "     --------------                           8.8/24.0 MB 3.9 MB/s eta 0:00:04\n",
      "     --------------                           8.9/24.0 MB 3.9 MB/s eta 0:00:04\n",
      "     ---------------                          9.2/24.0 MB 3.9 MB/s eta 0:00:04\n",
      "     ---------------                          9.5/24.0 MB 3.9 MB/s eta 0:00:04\n",
      "     ----------------                         9.7/24.0 MB 3.9 MB/s eta 0:00:04\n",
      "     ----------------                         10.1/24.0 MB 3.9 MB/s eta 0:00:04\n",
      "     -----------------                        10.3/24.0 MB 4.1 MB/s eta 0:00:04\n",
      "     -----------------                        10.5/24.0 MB 4.2 MB/s eta 0:00:04\n",
      "     ------------------                       10.9/24.0 MB 4.5 MB/s eta 0:00:03\n",
      "     ------------------                       11.2/24.0 MB 4.5 MB/s eta 0:00:03\n",
      "     -------------------                      11.5/24.0 MB 4.5 MB/s eta 0:00:03\n",
      "     -------------------                      11.7/24.0 MB 4.5 MB/s eta 0:00:03\n",
      "     -------------------                      11.9/24.0 MB 4.4 MB/s eta 0:00:03\n",
      "     --------------------                     12.2/24.0 MB 4.4 MB/s eta 0:00:03\n",
      "     --------------------                     12.5/24.0 MB 4.5 MB/s eta 0:00:03\n",
      "     ---------------------                    12.7/24.0 MB 4.5 MB/s eta 0:00:03\n",
      "     ---------------------                    13.0/24.0 MB 4.5 MB/s eta 0:00:03\n",
      "     ---------------------                    13.2/24.0 MB 4.4 MB/s eta 0:00:03\n",
      "     ----------------------                   13.3/24.0 MB 4.3 MB/s eta 0:00:03\n",
      "     ----------------------                   13.7/24.0 MB 4.4 MB/s eta 0:00:03\n",
      "     -----------------------                  14.0/24.0 MB 4.4 MB/s eta 0:00:03\n",
      "     -----------------------                  14.2/24.0 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------                 14.6/24.0 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------                 14.8/24.0 MB 4.5 MB/s eta 0:00:03\n",
      "     -------------------------                15.2/24.0 MB 4.4 MB/s eta 0:00:02\n",
      "     -------------------------                15.4/24.0 MB 4.4 MB/s eta 0:00:02\n",
      "     --------------------------               15.6/24.0 MB 4.4 MB/s eta 0:00:02\n",
      "     --------------------------               15.9/24.0 MB 4.4 MB/s eta 0:00:02\n",
      "     --------------------------               16.1/24.0 MB 4.3 MB/s eta 0:00:02\n",
      "     ---------------------------              16.3/24.0 MB 4.3 MB/s eta 0:00:02\n",
      "     ---------------------------              16.6/24.0 MB 4.3 MB/s eta 0:00:02\n",
      "     ----------------------------             16.9/24.0 MB 4.3 MB/s eta 0:00:02\n",
      "     ----------------------------             17.2/24.0 MB 4.3 MB/s eta 0:00:02\n",
      "     -----------------------------            17.5/24.0 MB 4.3 MB/s eta 0:00:02\n",
      "     -----------------------------            17.7/24.0 MB 4.3 MB/s eta 0:00:02\n",
      "     ------------------------------           18.0/24.0 MB 4.2 MB/s eta 0:00:02\n",
      "     ------------------------------           18.2/24.0 MB 4.2 MB/s eta 0:00:02\n",
      "     -------------------------------          18.7/24.0 MB 4.2 MB/s eta 0:00:02\n",
      "     -------------------------------          19.0/24.0 MB 4.2 MB/s eta 0:00:02\n",
      "     --------------------------------         19.3/24.0 MB 4.3 MB/s eta 0:00:02\n",
      "     --------------------------------         19.6/24.0 MB 4.2 MB/s eta 0:00:02\n",
      "     ---------------------------------        19.9/24.0 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------        20.2/24.0 MB 4.2 MB/s eta 0:00:01\n",
      "     ----------------------------------       20.6/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------------------       20.9/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     -----------------------------------      21.2/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     -----------------------------------      21.6/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------------     21.9/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------------     22.1/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------------    22.4/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     -------------------------------------    22.7/24.0 MB 4.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   23.0/24.0 MB 4.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   23.1/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     --------------------------------------   23.3/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.5/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.6/24.0 MB 4.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.7/24.0 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.8/24.0 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  24.0/24.0 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  24.0/24.0 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  24.0/24.0 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 24.0/24.0 MB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.21.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\user\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Collecting keras\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "                                              0.0/1.7 MB ? eta -:--:--\n",
      "                                              0.0/1.7 MB 660.6 kB/s eta 0:00:03\n",
      "     -                                        0.1/1.7 MB 657.6 kB/s eta 0:00:03\n",
      "     ---                                      0.1/1.7 MB 774.0 kB/s eta 0:00:03\n",
      "     -----                                    0.2/1.7 MB 1.1 MB/s eta 0:00:02\n",
      "     --------                                 0.4/1.7 MB 1.2 MB/s eta 0:00:02\n",
      "     -----------                              0.5/1.7 MB 1.3 MB/s eta 0:00:01\n",
      "     -------------                            0.6/1.7 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------                       0.8/1.7 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------                    0.9/1.7 MB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------                 1.1/1.7 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------              1.2/1.7 MB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------          1.4/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "     ----------------------------------       1.5/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "     -------------------------------------    1.6/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "Successfully installed keras-2.12.0\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.2-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "                                              0.0/10.8 MB ? eta -:--:--\n",
      "                                              0.0/10.8 MB ? eta -:--:--\n",
      "                                             0.0/10.8 MB 325.1 kB/s eta 0:00:34\n",
      "                                             0.1/10.8 MB 456.6 kB/s eta 0:00:24\n",
      "                                             0.2/10.8 MB 701.4 kB/s eta 0:00:16\n",
      "                                             0.2/10.8 MB 701.4 kB/s eta 0:00:16\n",
      "                                             0.3/10.8 MB 682.7 kB/s eta 0:00:16\n",
      "     -                                       0.4/10.8 MB 791.2 kB/s eta 0:00:14\n",
      "     -                                       0.5/10.8 MB 853.3 kB/s eta 0:00:13\n",
      "     -                                       0.5/10.8 MB 873.0 kB/s eta 0:00:12\n",
      "     --                                      0.7/10.8 MB 990.5 kB/s eta 0:00:11\n",
      "     --                                      0.8/10.8 MB 996.7 kB/s eta 0:00:11\n",
      "     ---                                      0.9/10.8 MB 1.1 MB/s eta 0:00:10\n",
      "     ---                                      1.0/10.8 MB 1.1 MB/s eta 0:00:09\n",
      "     ----                                     1.1/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     ----                                     1.2/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     -----                                    1.4/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     -----                                    1.4/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     -----                                    1.5/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     -----                                    1.6/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     ------                                   1.7/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ------                                   1.7/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ------                                   1.8/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     ------                                   1.9/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     -------                                  2.0/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     -------                                  2.1/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     --------                                 2.2/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     --------                                 2.3/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ---------                                2.5/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ---------                                2.7/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ----------                               2.8/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     -----------                              3.0/10.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -----------                              3.2/10.8 MB 1.5 MB/s eta 0:00:06\n",
      "     ------------                             3.3/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ------------                             3.4/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ------------                             3.4/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ------------                             3.5/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     -------------                            3.5/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     -------------                            3.7/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     -------------                            3.8/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------                           3.8/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------                           3.9/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------                          4.1/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------                          4.2/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------                         4.3/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ----------------                         4.4/10.8 MB 1.5 MB/s eta 0:00:05\n",
      "     -----------------                        4.6/10.8 MB 1.5 MB/s eta 0:00:04\n",
      "     -----------------                        4.7/10.8 MB 1.6 MB/s eta 0:00:04\n",
      "     ------------------                       4.9/10.8 MB 1.6 MB/s eta 0:00:04\n",
      "     ------------------                       5.0/10.8 MB 1.6 MB/s eta 0:00:04\n",
      "     -------------------                      5.2/10.8 MB 1.6 MB/s eta 0:00:04\n",
      "     -------------------                      5.2/10.8 MB 1.6 MB/s eta 0:00:04\n",
      "     --------------------                     5.4/10.8 MB 1.6 MB/s eta 0:00:04\n",
      "     --------------------                     5.6/10.8 MB 1.6 MB/s eta 0:00:04\n",
      "     ---------------------                    5.7/10.8 MB 1.6 MB/s eta 0:00:04\n",
      "     ----------------------                   6.0/10.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ----------------------                   6.1/10.8 MB 1.7 MB/s eta 0:00:03\n",
      "     -----------------------                  6.3/10.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ------------------------                 6.6/10.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ------------------------                 6.7/10.8 MB 1.7 MB/s eta 0:00:03\n",
      "     -------------------------                7.0/10.8 MB 1.8 MB/s eta 0:00:03\n",
      "     --------------------------               7.2/10.8 MB 1.8 MB/s eta 0:00:02\n",
      "     ---------------------------              7.5/10.8 MB 1.8 MB/s eta 0:00:02\n",
      "     ----------------------------             7.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ----------------------------             7.8/10.8 MB 1.8 MB/s eta 0:00:02\n",
      "     -----------------------------            8.0/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------------------------           8.1/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------          8.4/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------          8.6/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     --------------------------------         8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ---------------------------------        9.0/10.8 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------        9.1/10.8 MB 1.9 MB/s eta 0:00:01\n",
      "     ----------------------------------       9.4/10.8 MB 2.0 MB/s eta 0:00:01\n",
      "     -----------------------------------      9.5/10.8 MB 2.0 MB/s eta 0:00:01\n",
      "     -----------------------------------      9.6/10.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------     10.0/10.8 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------    10.2/10.8 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------------   10.3/10.8 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------------------------------   10.4/10.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.6/10.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.8/10.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.8/10.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.8/10.8 MB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "                                              0.0/341.8 kB ? eta -:--:--\n",
      "     -------                                 61.4/341.8 kB 1.6 MB/s eta 0:00:01\n",
      "     ------------                         122.9/341.8 kB 901.1 kB/s eta 0:00:01\n",
      "     ------------------                   174.1/341.8 kB 952.6 kB/s eta 0:00:01\n",
      "     -------------------------------        286.7/341.8 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 341.8/341.8 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\User\\\\anaconda3\\\\Lib\\\\site-packages\\\\~andas\\\\_libs\\\\algos.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim --upgrade\n",
    "!pip install keras --upgrade\n",
    "!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2660da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier,LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier,LogisticRegression\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,Input,SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721b9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/User/Downloads/Twitter_Data.csv\", encoding = 'latin-1', nrows=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba99ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>the able capable and innovative enough look in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>but you questioned modi jis headgears giving t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>let anyone give single reason should not vote ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>anpad ankitaise bolegaaadont surprise others t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>then shld have election between mms and modi i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text  category\n",
       "1354  the able capable and innovative enough look in...         1\n",
       "1357  but you questioned modi jis headgears giving t...         1\n",
       "794   let anyone give single reason should not vote ...         1\n",
       "1209  anpad ankitaise bolegaaadont surprise others t...         1\n",
       "828   then shld have election between mms and modi i...         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Sample\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae72f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text    1\n",
       "category      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NA Values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b578ab8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>868</td>\n",
       "      <td>43.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>706</td>\n",
       "      <td>35.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>426</td>\n",
       "      <td>21.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total  Percentage\n",
       "category                   \n",
       " 1          868        43.4\n",
       " 0          706        35.3\n",
       "-1          426        21.3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of different classes in sentiment\n",
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=[\"Total\",\"Percentage\"])\n",
    "count_values_in_column(df,\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529b8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segrating based on different sentiments\n",
    "df_negative = df[df[\"category\"]==-1]\n",
    "df_positive = df[df[\"category\"]==1]\n",
    "df_neutral = df[df[\"category\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9a9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "# Initializing Lists\n",
    "corpus = []\n",
    "words = []\n",
    "for i in range(0, len(df)):\n",
    "    # Removing characters other than letters\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(df[\"clean_text\"][i]))\n",
    "    # Lowering the case all the text\n",
    "    review = review.lower()\n",
    "    # Splitting into words\n",
    "    review = review.split()\n",
    "    # Applying Stemming\n",
    "    stemmed = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    # Joining words\n",
    "    review = ' '.join(stemmed)\n",
    "    # Appending all tweets to a list after preprocessing\n",
    "    corpus.append(review)\n",
    "    # Appending all words for word embeddings\n",
    "    words.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66670f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk nonsens continu drama vote modi',\n",
       " 'say vote modi welcom bjp told rahul main campaign modi think modi relax',\n",
       " 'ask support prefix chowkidar name modi great servic confus read crustal clear crass filthi nonsens see abus come chowkidar',\n",
       " 'answer among power world leader today trump putin modi may',\n",
       " 'kiya tho refresh maarkefir comment karo',\n",
       " 'surat women perform yagna seek divin grace narendra modi becom',\n",
       " 'come cabinet scholar like modi smriti hema time introspect',\n",
       " 'upcom elect india saga go import pair look current modi lead govt elect deal brexit combin weekli look juici bear imho',\n",
       " 'gandhi gay modi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corpus sample\n",
    "corpus[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec678b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legth of Corpus: 2000\n"
     ]
    }
   ],
   "source": [
    "# Length \n",
    "print(\"Legth of Corpus:\",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1b04357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating created corpus in our dataframe\n",
    "df[\"clean_text\"] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "559184aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NA Values and resetting index\n",
    "df = df.dropna()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf862b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index         0\n",
       "clean_text    0\n",
       "category      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NA Values after corpus updations\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42fe27e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting stemmed sentences\n",
    "df[[\"clean_text\",\"category\"]].to_csv(\"stemmed.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df62443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stemmed sentences\n",
    "df_stemmed = pd.read_csv(\"stemmed.csv\")\n",
    "# Extracting corpus\n",
    "corpus = list(df_stemmed[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e77ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying TFIDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000,ngram_range=(1,3))\n",
    "X_tfidf = tfidf.fit_transform(df[\"clean_text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f207237c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aadhaar</th>\n",
       "      <th>aap</th>\n",
       "      <th>abe</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abp</th>\n",
       "      <th>absolut</th>\n",
       "      <th>absolut hate</th>\n",
       "      <th>absolut hate congress</th>\n",
       "      <th>absolut right</th>\n",
       "      <th>...</th>\n",
       "      <th>yogi</th>\n",
       "      <th>yogi adityanath</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth victim</th>\n",
       "      <th>youth victim demo</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>zone</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aadhaar  aap  abe  abil  abl  abp  absolut  absolut hate  \\\n",
       "0      0.0  0.0  0.0   0.0  0.0  0.0      0.0           0.0   \n",
       "1      0.0  0.0  0.0   0.0  0.0  0.0      0.0           0.0   \n",
       "2      0.0  0.0  0.0   0.0  0.0  0.0      0.0           0.0   \n",
       "3      0.0  0.0  0.0   0.0  0.0  0.0      0.0           0.0   \n",
       "4      0.0  0.0  0.0   0.0  0.0  0.0      0.0           0.0   \n",
       "\n",
       "   absolut hate congress  absolut right  ...  yogi  yogi adityanath  your  \\\n",
       "0                    0.0            0.0  ...   0.0              0.0   0.0   \n",
       "1                    0.0            0.0  ...   0.0              0.0   0.0   \n",
       "2                    0.0            0.0  ...   0.0              0.0   0.0   \n",
       "3                    0.0            0.0  ...   0.0              0.0   0.0   \n",
       "4                    0.0            0.0  ...   0.0              0.0   0.0   \n",
       "\n",
       "   youth  youth victim  youth victim demo  youtub   yr  zone  output  \n",
       "0    0.0           0.0                0.0     0.0  0.0   0.0      -1  \n",
       "1    0.0           0.0                0.0     0.0  0.0   0.0       0  \n",
       "2    0.0           0.0                0.0     0.0  0.0   0.0       1  \n",
       "3    0.0           0.0                0.0     0.0  0.0   0.0       1  \n",
       "4    0.0           0.0                0.0     0.0  0.0   0.0       1  \n",
       "\n",
       "[5 rows x 5001 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Independent Variable\n",
    "X = df_stemmed[\"clean_text\"]\n",
    "# Dependent Varible\n",
    "Y=df_stemmed[\"category\"]\n",
    "df_tfidf = pd.DataFrame(X_tfidf,columns = tfidf.get_feature_names())\n",
    "df_tfidf[\"output\"] = Y\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1227c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test Split\n",
    "X_train_tfidf,X_test_tfidf,Y_train_tfidf,Y_test_tfidf = train_test_split(X_tfidf,Y,test_size=0.33,random_state = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "498db56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Model\n",
    "classfier_tfidf = MultinomialNB(alpha=0.1)\n",
    "# Fitting data\n",
    "classfier_tfidf.fit(X_train_tfidf,Y_train_tfidf)\n",
    "# Prediction on test data\n",
    "Y_pred_tfidf = classfier_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c76500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Model\n",
    "logistic_tfidf = LogisticRegression(solver='liblinear')\n",
    "# Fitting data\n",
    "logistic_tfidf.fit(X_train_tfidf,Y_train_tfidf)\n",
    "# Prediction on test data\n",
    "Y_pred_logistic_tfidf = logistic_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f035088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_log_tfidf = accuracy_score(Y_test_tfidf,Y_pred_logistic_tfidf)\n",
    "classification_log_tfidf = classification_report(Y_test_tfidf,Y_pred_logistic_tfidf)\n",
    "confusion_matrix_log_tfidf = confusion_matrix(Y_test_tfidf,Y_pred_logistic_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49c6e983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Logistic Regression: \n",
      "\n",
      " \n",
      " Accuracy :  0.5818181818181818 \n",
      " Classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.08      0.15       143\n",
      "           0       0.69      0.55      0.61       243\n",
      "           1       0.53      0.87      0.66       274\n",
      "\n",
      "    accuracy                           0.58       660\n",
      "   macro avg       0.64      0.50      0.47       660\n",
      "weighted avg       0.63      0.58      0.53       660\n",
      " \n",
      " Confusion matrix \n",
      " [[ 12  30 101]\n",
      " [  0 133 110]\n",
      " [  5  30 239]]\n"
     ]
    }
   ],
   "source": [
    "print(\"For Logistic Regression: \\n\")\n",
    "print(\" \\n Accuracy : \",acc_log_tfidf,\"\\n\",\"Classification report \\n\",classification_log_tfidf,\"\\n\",\"Confusion matrix \\n\",confusion_matrix_log_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1f39106",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_tfidf = accuracy_score(Y_test_tfidf,Y_pred_tfidf)\n",
    "classification_tfidf = classification_report(Y_test_tfidf,Y_pred_tfidf)\n",
    "confusion_matrix_tfidf = confusion_matrix(Y_test_tfidf,Y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "265c189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Mutinomial Naive Bayes: \n",
      "\n",
      " \n",
      " Accuracy :  0.55 \n",
      " Classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.42      0.36      0.39       143\n",
      "           0       0.67      0.47      0.55       243\n",
      "           1       0.54      0.72      0.62       274\n",
      "\n",
      "    accuracy                           0.55       660\n",
      "   macro avg       0.54      0.52      0.52       660\n",
      "weighted avg       0.56      0.55      0.54       660\n",
      " \n",
      " Confusion matrix \n",
      " [[ 52  20  71]\n",
      " [ 31 114  98]\n",
      " [ 40  37 197]]\n"
     ]
    }
   ],
   "source": [
    "print(\"For Mutinomial Naive Bayes: \\n\")\n",
    "print(\" \\n Accuracy : \",acc_tfidf,\"\\n\",\"Classification report \\n\",classification_tfidf,\"\\n\",\"Confusion matrix \\n\",confusion_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fef1cc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[774, 2530, 401, 14, 464, 1238],\n",
       " [817, 464, 1238, 1976, 2026, 4759, 1166, 4894, 336, 1238, 869, 1238, 4018]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cloning tweets\n",
    "messages=X.copy()\n",
    "# Setting parameter for padding and sequential modelling\n",
    "voc_size = 5000\n",
    "embedding_vector_features = 200\n",
    "sent_length = 200\n",
    "lstm_out = 128\n",
    "# Tokenization of all words in the vocabulary for all tweets\n",
    "onehot_repr=[one_hot(words,voc_size)for words in df[\"clean_text\"]]\n",
    "onehot_repr[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ac0feb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1238 3489 2942 ...    0    0    0]\n",
      " [ 774 2530  401 ...    0    0    0]\n",
      " [ 817  464 1238 ...    0    0    0]\n",
      " ...\n",
      " [2510  464 2262 ...    0    0    0]\n",
      " [  90 3126 1030 ...    0    0    0]\n",
      " [ 760 4635 1238 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Applying Post Padding\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='post',maxlen=sent_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e3519c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 200), (2000,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing Negative values to positive\n",
    "Y = [2 if x == -1 else x for x in Y]\n",
    "\n",
    "# Converting list to arrays\n",
    "X_final=np.array(embedded_docs)\n",
    "y_final=np.array(Y)\n",
    "# Shape of X,Y\n",
    "X_final.shape,y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e33711d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 40)          200000    \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 200)              112800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               25728     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 346,979\n",
      "Trainable params: 346,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "X_train_embed, X_test_embed, Y_train_embed, Y_test_embed = train_test_split(X_final, y_final, test_size=0.33, random_state=27)\n",
    "# Sequential Model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Input(shape=(None,)))\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(voc_size, embedding_vector_features, input_length = sent_length))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# model.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29b6c633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "6/6 [==============================] - 13s 1s/step - loss: 1.0871 - accuracy: 0.4246 - val_loss: 1.0671 - val_accuracy: 0.4152\n",
      "Epoch 2/2\n",
      "6/6 [==============================] - 4s 740ms/step - loss: 1.0541 - accuracy: 0.4448 - val_loss: 1.0490 - val_accuracy: 0.4152\n"
     ]
    }
   ],
   "source": [
    "# Compling model and running\n",
    "model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "history = model.fit(X_train_embed,Y_train_embed,validation_data=(X_test_embed,Y_test_embed),\n",
    "                    epochs=2, batch_size=256,\n",
    "                    verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "729b3963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 3s 55ms/step\n",
      "{'loss': [1.0870935916900635, 1.0541021823883057], 'accuracy': [0.4246268570423126, 0.44477611780166626], 'val_loss': [1.0670653581619263, 1.0489977598190308], 'val_accuracy': [0.4151515066623688, 0.4151515066623688]}\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_embed)\n",
    "print(history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
